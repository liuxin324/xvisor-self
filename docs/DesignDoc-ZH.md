
# Xvisor 设计文档

本文档提供了 Xvisor 设计的高层视图.它为开发者提供了关于 Xvisor 的非常重要的洞见，以便他们可以贡献给 Xvisor 的任何部分，甚至为 Xvisor 编写架构支持.

## 第1章: 建模虚拟机

虚拟机（VM）是对物理机器（即计算机系统）的软件仿真/模拟，能够执行程序或操作系统.

虚拟机分为两大类（基于它们的用途）: 

* 系统虚拟机: 系统虚拟机提供一个完整的系统平台，支持完整操作系统（OS）的执行.
* 进程虚拟机: 进程虚拟机设计用于运行单一程序，这意味着它支持单一进程.

虚拟机的一个基本特性是，运行在内部的软件仅限于虚拟机提供的资源和抽象 —— 它不能突破其虚拟世界.

（引用: 参考维基百科上关于“虚拟机”和“Hypervisor”的页面）

Xvisor 是一个硬件辅助的系统虚拟化软件（即使用硬件加速实现系统虚拟机），直接运行在宿主机器（即物理机器/硬件）上.简而言之，我们可以说 Xvisor 是一个原生（或第一类型）Hypervisor（或虚拟机监视器）.

在Xvisor中,我们将系统虚拟机实例称为“客户”实例，将系统虚拟机的虚拟CPU称为“VCPU”实例.
此外，属于客户的VCPU被称为“普通VCPU”，不属于任何客户的VCPU被称为“孤儿VCPU”.
Xvisor 为各种背景处理和运行管理守护程序创建孤儿 VCPUs.

任何现代 CPU 架构至少有两种特权模式: 用户模式和监督模式.
用户模式具有最低特权，监督模式具有最高特权.
Xvisor 在用户模式下运行普通 VCPUs，在监督模式下运行孤儿 VCPUs（注意: 特定于架构的代码必须区别对待普通和孤儿 VCPUs）.

下面的图1清晰地展示了 Xvisor 实现的系统虚拟机模型.

+--------------------------+                  +--------------------------+
|          Guest_0         |                  |          Guest_N         |
+--------------------------+                  +--------------------------+
| +--------+    +--------+ |                  | +--------+    +--------+ |
| |        |    |        | |                  | |        |    |        | |
| | VCPU_0 | .. | VCPU_M | |                  | | VCPU_0 | .. | VCPU_K | |
| |        |    |        | |                  | |        |    |        | |
| +--------+    +--------+ | ................ | +--------+    +--------+ |
+--------------------------+                  +--------------------------+
|       Address Space      |                  |       Address Space      |
|+--------+ +-----+ +-----+|                  |+--------+ +-----+ +-----+|
|| Memory | | PIC | | PIT ||                  || Memory | | PIC | | PIT ||
|+--------+ +-----+ +-----+|                  |+--------+ +-----+ +-----+|
| +-----+ +------+ +-----+ |                  | +-----+ +------+ +-----+ |
| | ROM | | UART | | LCD | |                  | | ROM | | UART | | LCD | |
| +-----+ +------+ +-----+ |                  | +-----+ +------+ +-----+ |
+--------------------------+                  +--------------------------+
+------------------------------------------------------------------------+
|                                                                        |
|                 eXtensible Versatile hypervISOR (Xvisor)               |
|                                                                        |
|  +----------------+  +----------+  +----------------+  +------------+  |
|  |     Guest      |  |  Command |  |  Virtualized   |  | Management |  |
|  |     Device     |  |  Manager |  |      I/O       |  |  Daemons   |  |
|  |   Emulators    |  +----------+  |   Frameworks   |  +------------+  |
|  +----------------+                +----------------+                  |
|                                                                        |
|  +---------------+  +------------+  +---------------+  +------------+  |
|  |  Hypervisor   |  | Hypervisor |  |   Hypervisor  |  | Hypervisor |  |
|  | Configuration |  |   Manager  |  | Load Balancer |  | Scheduler  |  |
|  +---------------+  +------------+  +---------------+  +------------+  |
|                                                                        |
|  +----------------+  +----------------+  +----------+  +------------+  |
|  |    Storage     |  |    Network     |  | Standard |  | Hypervisor |  |
|  | Virtualization |  | Virtualization |  |    I/O   |  |   Timer    |  |
|  +----------------+  +----------------+  +----------+  +------------+  |
|                                                         +-----------+  |
|                                                        +-----------+|  |
|  +---------------+  +--------------+  +------------+  +-----------+||  |
|  |     Misc      |  |     Host     |  |    Host    |  |  Orphan   |||  |
|  |    Driver     |  |    Device    |  |   Memory   |  |   VCPUs   ||+  |
|  |   Frameworks  |  |    Drivers   |  | Management |  | (Threads) |+   |
|  +---------------+  +--------------+  +------------+  +-----------+    |
|                                                                        |
+------------------------------------------------------------------------+
+------------------------------------------------------------------------+
|                                                                        |
|            Host Machine (Host CPU + Host Memory + Host Devices)        |
|                                                                        |
+------------------------------------------------------------------------+

                                [figure-1]

## 第2章: Hypervisor 配置

在“早期”，为系统配置操作系统（OS）是简单的，因为它有一个单一的中央处理单元（CPU）核心执行操作系统（OS）.然而，在当今世界，功能强大的单核和多核处理器实际上可以配置成许多不同的配置.一个多核处理器可以由一个单一的对称多处理（SMP）操作系统管理，该系统管理所有的核心.或者，每个核心可以在一个非对称多处理（AMP）配置中被赋予给一个单独的操作系统.SMP 和 AMP 都有它们的挑战和优势.例如，SMP 并不总是根据工作负载而良好地扩展.对于 AMP 而言，配置哪个操作系统可以访问哪个设备可能很难.操作系统假设它们完全控制它们检测到的硬件设备.通常，在 AMP 情况下，这会造成冲突.Xvisor 提供技术来分区或虚拟化在构建系统的多个操作系统之间的处理核心、内存和设备.

Xvisor 以称为“设备树”的树状数据结构形式维护其配置，以简化在单核或多核系统上运行 Xvisor 的配置任务.它高度受到 Linux 内核的 of_platform 使用的设备树源（DTS）的启发.因此，系统设计者可以利用各种配置——包括 AMP、SMP 和核心虚拟化的混合——来构建他们的下一代系统.

在 Linux 中，如果一个架构（例如 PowerPC）使用 of_platform，那么在启动时 Linux 内核将期望从引导加载程序获取一个 DTB 文件（扁平设备树文件）.DTB 文件是通过使用 DTC（设备树编译器）编译 DTS（设备树源）生成的二进制文件.一个启用了 of_platform 的 Linux 内核只会探测那些与 DTB 文件中的设备树提到的设备兼容或匹配的驱动.与 Linux 的 of_platform 使用 DTB 文件不同，对 Xvisor 来说并不是必需的.Xvisor 的特定于架构的代码（或更准确地说是特定于板的代码）可以从各种来源填充设备树，例如 DTB 或 ACPI 表.简单来说，Xvisor 中的设备树是用于管理 hypervisor 配置的数据结构.

(注: 有关 PowerPC Linux 内核使用的设备树语法的更多信息，请参考 <https://www.power.org/resources/downloads/Power_ePAPR_APPROVED_v1.0.pdf>)

虽然 Xvisor 设备树只是一个数据结构，但在更新/填充 Xvisor 设备树时必须确保以下约束: 

* 节点名称: 它必须只包含以下字符之一，
      -> 数字: [0-9]
      -> 小写字母: [a-z]
      -> 大写字母: [A-Z]
      -> 下划线: _
      -> 破折号: -
* 属性名称: 它必须只包含以下字符之一，
      -> 数字: [0-9]
      -> 小写字母: [a-z]
      -> 大写字母: [A-Z]
      -> 下划线: _
      -> 破折号: -
      -> 井号: #
* 属性字符串值: 字符串属性值必须以 NULL 字符结束（即 '\0' 或字符值 0）.对于字符串列表，每个字符串必须由一个 NULL 字符准确分隔.
* 属性 32 位无符号值: 一个 32 位整数值必须根据宿主 CPU 架构的字节序表示为大端格式或小端格式.
* 属性 64 位无符号值: 一个 64 位整数值必须根据宿主 CPU 架构的字节序表示为大端格式或小端格式.
(注: 特定于架构的代码在填充设备树时必须确保满足上述约束)
(注: 有关 Xvisor 使用的标准属性，请参考源代码.)

下面的图2显示了第1章图1中所示的 hypervisor 设置的设备树表示.

  (Root)
+--------+
|        |
+--------+
    |
    |          (Host CPUs)
    |          +--------+
    |----------|  cpus  |
    |          +--------+
    |              |
    |              |          +--------+
    |              +----------|  cpu0  |
    |              |          +--------+
    |              |              .
    |              |              .
    |              |              .
    |              |          +--------+
    |              +----------|  cpuL  |
    |                         +--------+
    |
    |        (Host Hardware)
    |          +--------+
    +----------|  ....  |
    |          +--------+
    |
    |     (General Configuration)
    |          +--------+
    +----------|  vmm   |
    |          +--------+
    |
    |     (Guests Instances)
    |          +--------+
    +----------| guests |
               +--------+
                   |
                   |           (Guest)
                   |          +--------+
                   +----------| guest0 |
                   |          +--------+
                   |              |        (Guest VCPUs)
                   |              |          +--------+
                   |              |----------| vcpus  |
                   |              |          +--------+
                   |              |              |
                   |              |              |            (VCPU)
                   |              |              |          +--------+
                   |              |              +----------| vcpu0  |
                   |              |              |          +--------+
                   |              |              |              .
                   |              |              |              .
                   |              |              |              .
                   |              |              |            (VCPU)
                   |              |              |          +--------+
                   |              |              +----------| vcpuM  |
                   |              |                         +--------+
                   |              |
                   |              |     (Guest Address Space)
                   |              |          +--------+
                   |              +----------| aspace |
                   |                         +--------+
                   |                             |
                   |                             |        (Guest Region)
                   |                             |          ----------
                   |                             +----------| Memory |
                   |                             |          ----------
                   |                             |
                   |                             |        (Guest Region)
                   |                             |          +--------+
                   |                             +----------|  PIC   |
                   |                             |          +--------+
                   |                             |
                   |                             |        (Guest Region)
                   |                             |          +--------+
                   |                             +----------|  PIT   |
                   |                             |          +--------+
                   |                             |
                   |                             |        (Guest Region)
                   |                             |          +--------+
                   |                             +----------|  UART  |
                   |                             |          +--------+
                   |                             |
                   |                             |        (Guest Region)
                   |                             |          +--------+
                   |                             +----------|  LCD   |
                   |                             |          +--------+
                   |                             |
                   |                             |        (Guest Region)
                   |                             |          ----------
                   |                             +----------|  ROM   |
                   |                                        +--------+
                   |
                   |           (Guest)
                   |          ----------
                   +----------| guestN |
                              ----------
                                  |        (Guest VCPUs)
                                  |          +--------+
                                  +----------| vcpus  |
                                  |          +--------+
                                  |              |
                                  |              |            (VCPU)
                                  |              |          +--------+
                                  |              +----------| vcpu0  |
                                  |              |          +--------+
                                  |              |              .
                                  |              |              .
                                  |              |              .
                                  |              |            (VCPU)
                                  |              |          +--------+
                                  |              +----------| vcpuK  |
                                  |                         +--------+
                                  |
                                  |     (Guest Address Space)
                                  |          +--------+
                                  +----------| aspace |
                                             +--------+
                                                 |
                                                 |        (Guest Region)
                                                 |          +--------+
                                                 +----------| Memory |
                                                 |          +--------+
                                                 |
                                                 |        (Guest Region)
                                                 |          +--------+
                                                 +----------|  PIC   |
                                                 |          +--------+
                                                 |
                                                 |        (Guest Region)
                                                 |          +--------+
                                                 +----------|  PIT   |
                                                 |          +--------+
                                                 |
                                                 |        (Guest Region)
                                                 |          ----------
                                                 +----------|  UART  |
                                                 |          +--------+
                                                 |
                                                 |        (Guest Region)
                                                 |          +--------+
                                                 +----------|  LCD   |
                                                 |          +--------+
                                                 |
                                                 |        (Guest Region)
                                                 |          +--------+
                                                 +----------|  ROM   |
                                                            +--------+
                                [figure-2]

默认情况下，Xvisor 将始终支持使用 DTS 配置设备树.它还包括一个取自 Linux 内核源代码的 DTC 编译器和一个
可供架构使用的轻量级DTB解析库（libfdt）用于填充 Xvisor 设备树的特定代码（或板特定代码）.

## 第3章: Hypervisor 计时器

像任何操作系统一样，hypervisor 也需要使用时间跟踪子系统来跟踪时间的流逝.我们将 Xvisor 的时间跟踪子系统称为 hypervisor 计时器.操作系统的时间跟踪子系统执行两个关键任务，分别是: 
1.跟踪时间的流逝: 实现这一点的传统方法是计算周期性中断，但这种方法非常不精确且分辨率低.一种更精确且开销更低的方法是使用 clocksource 设备（即自由运行的周期准确的硬件计数器）作为时间流逝的参考.
2.在未来安排事件: 为了实现这一点，操作系统将保持每个 CPU 的事件列表，根据它们的到期时间按升序排序.操作系统将使用每个 CPU 的 clockevent 设备（例如 PIT）逐一安排排序事件，首先是最早到期的事件.所有操作系统在如何维护每个 CPU 的排序事件列表上会有所不同.

与任何操作系统不同，对于 hypervisors 来说，时间跟踪尤其棘手，因为存在许多挑战.最明显的问题是，时间现在在主机和可能许多客户实例之间共享.客户操作系统从不获得 100% 的 CPU 执行时间，尽管实际上它可能会做出这样的假设.当禁用中断源时，它可能期望这一点保持非常精确的界限，但实际上只有其虚拟中断源被禁用，而机器仍然可以随时被抢占.这导致了一个问题，即真实时间的流逝、客户中断的注入以及相关的时钟源不再与真实时间完全同步.

对于遗留的客户操作系统来说，最直接的问题之一是，系统时间跟踪程序经常设计为通过计算周期性中断来跟踪时间.这些中断可能来自 PIT 或 RTC，但问题是相同的: 主机虚拟化引擎可能无法以适当的速率交付中断，因此客户时间可能会落后.如果选择了高中断率（如 1000 HZ），这将特别成问题，不幸的是，这是许多 Linux 客户的默认设置.

解决这个问题有三种方法: 
1.对于具有单独时间源以跟踪“墙上时钟”或“实际时间”的客户，可能简单地忽略这个问题可能不需要调整它们的中断来维持正确的时间.
2.如果这还不够，可能需要向客户注入额外的中断，以增加有效的中断率.这种方法在极端条件下导致复杂情况，其中宿主负载或客户延迟过多而无法补偿.
3.客户可能需要意识到丢失的时钟周期并内部补偿它们.尽管从理论上看这很有前景，但在 Linux 中实施这一政策极易出错，许多常用的 Linux 系统中分布着错误的丢失时钟周期补偿变体.

由此可见，hypervisor 必须以低开销、高精度和高分辨率跟踪时间流逝（即我们不能计算周期性中断来跟踪流逝的时间）.简而言之，hypervisor 中的时间跟踪必须是无滴答声且高分辨率的.
此外，hypervisor 中的 PIT 模拟器可能必须保留待处理的周期性中断的积压中断.

Xvisor 的 hypervisor 计时器子系统受到 Linux hrtimer 子系统的极大启发，并且完全无需定时器滴答.它提供以下功能: 

   1. 64位时间戳: 该时间戳代表自 Xvisor 启动以来经过的纳秒数.（即 Xvisor 的运行时间，以纳秒为单位）
   2. 计时器事件: 我们可以创建或销毁具有关联到期时间（以纳秒为单位）和到期回调处理程序的计时器事件.
   这些时间事件是一次性事件（即它们在到期时会自动停止），要拥有周期性计时器事件，我们必须从其到期回调处理程序手动重新启动计时器事件.

hypervisor 计时器需要特定于架构的代码为每个宿主 CPU 提供一个全局时钟源设备和一个时钟事件设备，以提供上述功能.

## 第4章: Hypervisor 管理器

Xvisor 中的 VCPU 和客户实例是由 Hypervisor 管理器创建和管理的.它还提供了建立在 hypervisor 调度器例程之上的 VCPU 状态变化、VCPU 统计信息以及 VCPU 宿主 CPU 变化的例程.

就像任何操作系统一样，Xvisor 中的一个 VCPU 实例有一个与架构相关的部分和一个与架构无关的部分.

VCPU 上下文的与架构相关的部分包括: 

1.架构寄存器: 只有在用户模式（或非特权模式）下才由处理器更新的寄存器.这些寄存器通常是通用寄存器和状态标志，由处理器自动更新（例如，比较标志、溢出标志、零标志等）.正常和孤儿 VCPUs 都需要他们自己的架构寄存器副本.我们将 VCPU 的架构寄存器称为 VCPU 结构的 “arch_regs_t *regs” 成员.

2.架构私有: 只有在监督模式（或特权模式）下才由处理器更新的寄存器.每当正常 VCPU 尝试读取/写入此类寄存器时，我们会收到一个异常，并且我们可以返回/更新其虚拟值.在大多数情况下，架构私有中还有一些额外的数据结构（如 MMU 上下文、影子 TLB、影子页表等）.孤儿 VCPUs 通常不需要架构私有，只有正常 VCPUs 需要它们.我们将 VCPU 的架构私有称为 VCPU 结构的 “void *arch_priv” 成员.

VCPU 上下文包括以下内容: 

  1. ID: Globally unique identification number
  2. SUBID: 在 parent Guest中唯一的识别号.（仅适用于正常 VCPUs）
  3. Name: 为此 VCPU 给定的名称.（仅适用于孤儿 VCPUs）
  4. Device Tree Node: 指向 VCPU 设备树节点的指针.（仅适用于正常 VCPUs）
  5. Is Normal: 标志显示此 VCPU 是正常还是孤儿.
  6. Is PowerOff: 标志显示 VCPU 在客户重置后是否应处于 RESET 状态.（仅适用于正常 VCPUs）
  7. Guest: Pointer to parent Guest.
  8. Start PC: 程序计数器的起始值
  9. Stack VA: 堆栈指针的起始值.对于孤儿 VCPU（或线程），这将是运行时堆栈，而对于正常 VCPU，这将是处理异常和中断的特殊堆栈.
  10. Stack Size: VCPU 堆栈的大小
  11. Sched Lock:用于保护动态调度器上下文的读写自旋锁.
  12. Host CPU: VCPU 将在其上运行的宿主 CPU.（使用调度锁保护）

-------------------------

调度器动态上下文
-------------------------

  13. Host CPU Affinity(宿主 CPU 亲和性): 表示 VCPU 可以运行的宿主 CPU 的掩码.（通过调度锁保护）
  14. State(状态): 当前 VCPU 状态.（通过调度锁保护）（下面解释）
  15. State Timestamp(状态时间戳): VCPU 进入其当前状态的时间戳.（通过调度锁保护）
  16. State Ready Nanosecs(状态就绪纳秒数): VCPU 在就绪状态下花费的时间.（通过调度锁保护）
  17. State Running Nanosecs(状态运行纳秒数): VCPU 在运行状态下花费的时间.（通过调度锁保护）
  18. State Paused Nanosecs(状态暂停纳秒数): VCPU 在暂停状态下花费的时间.（通过调度锁保护）
  19. State Halted Nanosecs(状态停止纳秒数): VCPU 在停止状态下花费的时间.（通过调度锁保护）
  20. Reset Count(重置计数): VCPU 被重置的次数.（通过调度锁保护）
  21. Reset Timestamp(重置时间戳): VCPU 处于重置状态时的时间戳.（通过调度锁保护）
  22. Preempt Count(抢占计数): VCPU 禁用抢占的次数.（通过调度锁保护）
  23. Resumed(已恢复): 标志显示 VCPU 在进入暂停状态之前是否在就绪或运行状态时被恢复.（通过调度锁保护）
  24. Private Scheduling Context(私有调度上下文): 此 VCPU 的调度策略的私有上下文.（通过调度锁保护）

------------------------

调度器静态上下文
------------------------

  25. Static Scheduling Parameters(静态调度参数): 在 VCPU 创建时提供的调度参数，将被调度策略使用.（例如，优先级、时间片、截止日期和周期性）
  26. Architecture specific context(架构特定上下文): 此 VCPU 的架构特定上下文.架构特定代码负责管理此上下文.
  27. Virtual IRQ Info.(虚拟 IRQ 信息): VCPU 虚拟中断的管理信息（例如，断言、解除断言、挂起和执行的计数）
  28. Waitqueue Info.(等待队列信息): 等待队列所需的信息.
  29. Device Emulation Context(设备仿真上下文): 每个 VCPU 所需的设备仿真框架的私有信息的指针.

VCPU 在任何给定时间点只能处于一种状态.以下是所有可能状态的简要描述: 

* 未知（UNKNOWN）: VCPU 不属于任何客户，也不是孤儿 VCPU.为了降低内存占用，我们根据最大 VCPU 数量预先分配内存，并将它们置于此状态.
* 重置（RESET）: VCPU 已初始化，正在等待某人将其踢到就绪状态.为了创建一个新的 VCPU，VCPU 调度器从预分配的 VCPU 中挑选一个处于未知状态的 VCPU 并初始化它.初始化后，新创建的 VCPU 被置于重置状态.
* 就绪（READY）: VCPU 准备在硬件上运行.
* 运行（RUNNING）: VCPU 当前正在硬件上运行.
* 暂停（PAUSED）: VCPU 已被停止，稍后可以恢复.VCPU 被置于此状态（通常由架构特定代码）时，它检测到 VCPU 处于空闲状态，可以被调度出去.
* 停止（HALTED）: VCPU 已被停止，无法恢复.VCPU 被置于此状态（通常由架构特定代码）时，该 VCPU 由于一些错误访问被停止.


VCPU 状态变化可以从各种位置发生，例如架构特定代码、某些 hypervisor 线程、调度器、某些模拟设备等.
不可能有一个详尽的所有可能需要 VCPU 状态变化的场景列表，但是 VCPU 状态变化必须严格遵循有限状态机，这由 hypervisor 调度器确保.

图3展示了 VCPU 状态变化的有限状态机.

                                           +---------+
                               [Reset]     |         |      [Halt]
                           +---------------|  HALTED |<-----------------+
                           |               |         |                  |
                           |               +---------+                  |
                           |                    A                       |
                           |                    | [Halt]                |
                           V                    |                       |
+---------+ [Create]  +---------+  [Kick]  +---------+ [Scheduler] +---------+
|         |---------->|         |--------->|         |------------>|         |
| UNKNOWN |           |  RESET  |          |  READY  |             | RUNNING |
|         |<----------|         |<---------|         |<------------|         |
+---------+ [Destroy] +---------+  [Reset] +---------+ [Scheduler] +---------+
                        A     A              A     |                 |     |
                        |     |     [Resume] |     | [Pause]         |     |
                        |     |              |     V                 |     |
                        |     |            +---------+               |     |
                        |     |            |         |               |     |
                        |     +------------|  PAUSED |<--------------+     |
                        |        [Reset]   |         |   [Pause]           |
                        |                  +---------+                     |
                        |                                                  |
                        +--------------------------------------------------+
                                             [Reset]

                                [figure-3]

每个 VCPU 的虚拟中断数量及其优先级由架构特定代码提供.任何虚拟中断的断言/解除断言都是从与架构无关的代码触发的.此外，VCPU 可以等待/暂停，直到下一次虚拟中断的断言.

一个客户实例包含以下内容: 

  1. ID: Globally unique identification number.
  2. Device Tree Node(设备树节点): 指向客户设备树节点的指针.
  3. VCPU Count(VCPU 数量): 属于此客户的 VCPU 实例数量.
  4. VCPU List(VCPU 列表): 属于此客户的 VCPU 实例列表.
  5. Guest Address Space Info(客户地址空间信息): 管理客户物理地址空间所需的信息.
  6. Arch Private(架构私有): 此客户的架构依赖上下文.

客户地址空间也是一个与架构无关的抽象，包含以下内容: 

  1. Device Tree Node(设备树节点): 指向客户地址空间设备树节点的指针.
  2. Guest: 指向拥有此客户地址空间的客户的指针.
  3. Region List: 一组“客户区域”.
  4. Device Emulation Context: 每个客户地址空间所需的设备仿真框架的私有信息的指针.

每个客户区域都有一个独特的客户物理地址（即客户 VCPU 可以访问区域的物理地址）和物理大小（即客户区域的大小）.此外，客户区域可以是以下三种形式之一: 

* 真实客户区域: 真实客户区域直接访问宿主机器设备/内存（例如，RAM，UART 等）.这种类型的区域直接将客户物理地址映射到宿主物理地址（即宿主机中的物理地址）.
* 虚拟客户区域: 虚拟客户区域访问模拟设备（例如，模拟 PIC，模拟定时器等）.这种类型的区域通常与模拟设备相关联.架构特定代码负责将虚拟客户区域的读/写访问重定向到 Xvisor 设备仿真框架.
* 别名客户区域: 别名客户区域通过另一个客户物理地址访问另一个客户区域.

## 第5章: Hypervisor 调度器

Xvisor 的 hypervisor 调度器在调度策略（或调度算法）方面是通用且可插拔的.每当从 hypervisor 管理器接收到 VCPU 状态变化的通知时，它会更新每个 CPU 的就绪队列.hypervisor 调度器使用每个 CPU 的 hypervisor 计时器事件来为 VCPU 分配时间片.当某个 CPU 的调度器计时器事件到期时，调度器将使用某种调度策略（或算法）找到下一个 VCPU，并为下一个 VCPU 配置调度器计时器事件.

对于 Xvisor 来说，一个普通 VCPU 是一个黑盒（即，任何东西都可能在 VCPU 上运行），异常或中断是唯一获取控制权的方式.每当我们执行 Xvisor 代码时，我们可能处于以下任一上下文中: 

* IRQ 上下文: 当服务来自宿主机某外部设备生成的中断时.
* 普通上下文: 代表普通 VCPU 在 Xvisor 中仿真某些功能或指令或仿真 IO 时.
* 孤儿上下文: 当以孤儿 VCPU 或线程运行 Xvisor 的某些部分时（注意: Hypervisor 线程稍后描述）.

与其他 hypervisor 不同，Xvisor 有一个称为普通上下文的特殊上下文.只有当 hypervisor 代表普通 VCPU 做某些事情（如处理异常、仿真 IO 等）时，才处于普通上下文.普通上下文是不可睡眠的，这意味着普通 VCPU 在普通上下文中不能被调度出去.实际上，只有当 Xvisor 退出 IRQ 上下文或普通上下文时，普通 VCPU 才会被调度出去.这有助于 Xvisor 确保处理异常或仿真 IO 的可预测延迟.

调度器借助架构特定的异常或中断处理程序跟踪当前执行上下文.

架构特定的 VCPU 上下文切换涉及的预期高级步骤如下: 

  1. 从堆栈（由架构特定的异常或中断处理程序保存）保存架构寄存器（或 arch_regs_t）到当前 VCPU 的架构寄存器（或 arch_regs_t）.
  2. 在堆栈上恢复下一个 VCPU 的架构寄存器（或 arch_regs_t）（当从异常或中断处理程序 C 代码返回时将被恢复）.
  3. 切换架构特定的 CPU 资源（如 MMU、浮点子系统等）的上下文.

调度器调用 VCPU 上下文切换的可能场景如下: 

  1. 当分配给当前 VCPU 的时间片到期时，我们调用 VCPU 上下文切换.我们称这种情况为 VCPU 抢占.
  2. 如果一个普通 VCPU 表现不当（即执行无效的寄存器/内存访问），那么架构特定代码可以检测到这种情况并使用 hypervisor 管理器的 API 停止/暂停负责的普通 VCPU.
  3. 一个孤儿 VCPU（或线程）选择自愿暂停（即睡眠）.
  4. 一个孤儿 VCPU（或线程）选择自愿放弃其时间片.
  5. VCPU 状态也可以从其他 VCPU 使用 hypervisor 管理器 API 更改.

我们可以从 Xvisor menuconfig 选项中选择不同的调度策略（或算法）. 任何调度策略（或算法）都会得到每个 VCPU 的以下调度信息: 

  1. Priority(优先级): VCPU 的优先级.数值越高，优先级越高.
  2. Time Slice(时间片): 一旦 VCPU 被调度，它必须获得的最小时间量（以纳秒为单位）.
  3. Deadline(截止日期): VCPU 必须被调度运行的最大时间量（以纳秒为单位）.
  4. Periodicity(周期性): VCPU 变为就绪或获得工作的频率（以纳秒为单位）.

目前，可用的调度策略（或算法）包括: 

  1. 固定优先级轮转（PRR）
  2. 固定优先级最早截止日期优先（PEDF）
(注: Xvisor 默认使用固定优先级轮转策略)

## 第6章: Hypervisor 负载均衡器

除了在每个宿主 CPU 上调度任务外，通用操作系统或 hypervisor 的调度器还必须在 SMP 宿主上的多个宿主 CPU 之间负载均衡任务数量.

与传统的通用操作系统不同，Xvisor 的 hypervisor 调度器不进行负载均衡.
相反，Xvisor hypervisor 调度器为每个宿主 CPU 保持单独的上下文，仅负责每个宿主 CPU 的调度工作.
在多个宿主 CPU 上平衡 VCPUs（普通或孤儿）的任务由 Xvisor 中的一个独立实体执行，称为 hypervisor 负载均衡器.

hypervisor 负载均衡器实现为在固定宿主 CPU 上运行的孤儿 VCPU（或线程）.
它将在 VCPU 创建时向 hypervisor 管理器提供关于为给定 VCPU 分配的“hcpu”的提示.
它还将定期在几秒钟的间隔内被调用，以跨宿主 CPU 平衡 VCPUs.
此外，hypervisor 负载均衡器的负载均衡策略是运行时可插拔的.
目前，我们有“粗略”的负载均衡策略，基于宿主 CPU 利用率来平衡 VCPUs.Xvisor 的用户可以根据自己的用例或宿主硬件编写自己的负载均衡策略.

总结来说，Xvisor 的 hypervisor 负载均衡器是一个可插拔的懒惰工作线程，负责在多个宿主 CPU 之间平衡 VCPUs.

## 第7章: Hypervisor 线程

Xvisor 中用于管理后台线程的框架称为 Hypervisor 线程.Xvisor 中的线程与孤儿 VCPU 没有区别，实际上每个线程都是围绕一个孤儿 VCPU 的封装.线程的最佳示例是我们的管理终端（mterm）.

在 Xvisor 中创建线程需要以下五个必需条件: 

  1. Name: 分配给此线程的名称.
  2. Function: 线程入口函数指针.
  3. Data: 作为参数传递给线程入口函数的任意数据的 void 指针.
  4. Priority: 线程（或底层孤儿 VCPU）的优先级.数值越高，优先级越高.
  5. Time Slice:一旦线程（或底层孤儿 VCPU）被调度，它必须获得的最小时间量（以纳秒为单位）.

我们不需要为每个线程显式创建堆栈，因为 hypervisor 管理器会在 VCPU 创建时自动为每个孤儿 VCPU（即线程）创建固定大小的堆栈.
所有线程的默认堆栈大小可以在编译时通过 Xvisor menuconfig 选项更改.

线程 ID、优先级和时间片与底层孤儿 VCPU 的 ID、优先级和时间片相同.

线程在任何时间点都可以处于以下状态之一: 

* 已创建（CREATED）: 线程刚刚创建，还未运行.
* 运行中（RUNNING）: 线程正在运行.
* 睡眠中（SLEEPING）: 线程正在等待队列中睡眠.
* 已停止（STOPPED）: 线程已停止.它要么被强制停止，要么已完成其任务.
（注: 孤儿 VCPU 状态可以直接映射到线程状态之一，因此要获取当前线程状态，我们查看底层孤儿 VCPU 的状态.）

对于线程间同步，我们有以下同步原语: 

  1. Spinlocks(自旋锁): 通常用于较小的临界区，以及 IRQ 上下文、普通上下文和孤儿上下文之间的同步.
  2. Completion: 当线程（或孤儿 VCPU）希望等待某个事件（例如，来自宿主设备的中断）发生时通常使用.
  3. Semaphore(信号量): 传统的信号量锁，允许线程（或孤儿 VCPU）在锁（或资源）不可用时睡眠.
  4. Mutex(互斥锁): 传统的互斥锁，允许线程（或孤儿 VCPU）在锁不可用时睡眠.

上述完成、信号量和互斥锁使用 Xvisor 等待队列来睡眠.只有线程（或孤儿 VCPU）可以在等待队列中睡眠，因为我们不能在 IRQ 和普通上下文中睡眠.
因此，只能在孤儿上下文中进行对完成、信号量和互斥锁的可睡眠操作.

## 第8章: 设备驱动框架

Xvisor 的设备驱动框架与 Linux 内核设备驱动模型在抽象和可用 API 方面非常相似.
与 Linux 设备驱动模型的相似性帮助 Xvisor 提供 Linux 兼容头文件以便设备驱动移植.
我们还有类似 Linux 的设备资源管理 API，Xvisor 使用它们来跟踪各种设备驱动使用的宿主资源.

框架定义了以下实体: 

vmm_bus
逻辑上代表一个总线，多个设备可以存在于这个总线上.
例如，平台（platform）、USB、SPI、I2C 等.

vmm_class
逻辑上代表由一组设备实现的功能类（CLASS）.
例如，vmm_chardev（字符设备）、vmm_blockdev（块设备）、vmm_netport（网络端口）、vmm_netswitch（网络交换机）等.

vmm_device
逻辑上代表一个设备（DEVICE），该设备要么位于一个总线上，要么是某个类的一部分，但不能同时是两者.
一个设备可以是其他设备的子设备，也可以拥有自己的子设备.如果设备 X 位于总线上并实现了类 A 和类 B 的功能，那么我们将有伪设备 XA 和 XB，它们分别属于类 A 和类 B，父设备为 X.

vmm_driver
逻辑上代表驻留在总线上的设备的设备驱动程序（DEVICE DRIVER）.我们只能向总线注册设备驱动程序，而不能向类注册.

上述定义的实体在运行时由各种 Xvisor 模块注册.Xvisor 中的默认总线是平台总线，这是一个伪总线，用于通过 Xvisor 设备树探测所有设备.vmm_chardev 是 Xvisor 中始终可用的默认类，因为标准 IO 子系统（稍后描述）和命令管理器（稍后描述）严重依赖于字符设备.

## 第9章: 设备仿真框架

设备仿真框架是任何 hypervisor 中最关键的组件之一.它帮助 hypervisor 为客户提供特定的虚拟硬件.
Xvisor 的设备仿真框架旨在灵活、轻量和快速.Xvisor 设备仿真框架最重要的实体是: vmm_emulator 和 vmm_emudev.
vmm_emulator 是设备仿真模块注册的设备仿真器的逻辑表示，而 vmm_emudev 是任何被仿真/直通的设备的逻辑表示.

在创建客户时，hypervisor 管理器将在设备仿真框架的帮助下为每个虚拟和直通客户区域创建一个 vmm_emudev 实例.
框架将尝试为每个新创建的 vmm_emudev 实例探测匹配的 vmm_emulator，如果框架未能找到匹配的 vmm_emulator 或 vmm_emulator 的探测函数返回错误，则客户创建失败.

除上述外，Xvisor 设备仿真框架还为中断控制器仿真器和 GPIO 控制器仿真器提供特殊支持，通过提供固定数量的客户 irq 线.
可以通过客户设备树在客户创建时指定客户 irq 线的总数.
中断控制器和 GPIO 控制器仿真器将提供回调函数来监视某些客户 irq 线的电平变化.
中断控制器仿真器将基于客户 irq 线的电平变化触发客户 VCPU 中断，而 GPIO 控制器仿真器将基于客户 irq 线的电平变化执行某些操作.

所有的 vmm_emulator 读/写回调都在普通上下文中调用，因此设备仿真器不能在读/写回调中睡眠或使用可睡眠锁，如果绝对必要睡眠，则设备仿真器将不得不使用后台工作线程（或孤儿 VCPUs）.
这种“在设备仿真上下文中无法睡眠”的策略帮助 Xvisor 确保设备仿真的可预测延迟，这对实时系统至关重要.

## 第10章: 标准 I/O

任何通用操作系统或实时操作系统都需要一种打印和扫描 ASCII 文本的方式.Xvisor 也不例外，因此我们在 Xvisor 中有标准 IO 子系统，它实现了各种形式的打印和扫描 API.

标准 I/O 子系统需要以下内容来完成其任务: 

1. "defterm" 函数来自架构特定代码
   打印和扫描 ASCII 文本的默认方式是使用架构特定代码提供的 defterm 函数.
   架构特定代码将为 defterm 提供 init、getc 和 putc 函数.
   这些 "defterm" 函数对所有架构都是必需的，但架构特定代码可以选择提供存根实现.

2. "defterm early" 打印函数来自架构特定代码
   标准 I/O 子系统作为启动过程的一部分被初始化.在标准 I/O 子系统初始化之前，有很多初始化工作需要完成，
   所以我们有来自架构特定代码的 defterm early 打印函数，如果 Xvisor 的某些部分在标准 I/O 子系统初始化之前尝试打印 ASCII 文本，
   将会调用此函数.架构独立代码包括了一个 defterm early 打印函数的弱实现，所以为架构特定代码提供这个函数完全是可选的.
   一般来说，"defterm early" 打印函数仅用于调试目的，并且在大多数架构上应默认禁用.

3. 字符设备
   标准 I/O 子系统可以从任何字符设备实例（即 vmm_chardev）打印/扫描字符.
   只有当没有为标准 I/O 设置字符设备实例时，它才会使用 "defterm" 函数.
   可以使用管理守护程序的各种标准 I/O 命令或通过调用标准 I/O 子系统的更改设备 API 来为标准 I/O 设置字符设备.
   不是必须对所有标准 I/O 交互使用相同的字符设备设置.
   实际上，标准 I/O 子系统提供了 API，可以在特定的字符设备实例上打印或扫描字符，
   这可能与已为标准 I/O 设置的字符设备实例不同.

除上述内容外，标准 I/O 子系统还提供了许多调试宏和堆栈跟踪打印 API.要打印堆栈跟踪，标准 I/O 子系统将再次依赖于架构特定代码.

## 第11章: 命令管理器

作为一个单体式 hypervisor，Xvisor 需要命令行界面，但命令行界面有几种传输介质，例如: 

1. 通过串行端口
2. 通过基于 VT-100 的帧缓冲区上的图形控制台
3. 通过网络通过 telnet 连接
4. ----- 还有更多 -----

Xvisor 命令管理器提供了一种与传输介质无关的管理和执行命令的方式，
这样我们就可以在各种管理守护程序中共享 Xvisor 命令，
无需进行任何更改.最重要的是，它提供了 API，
允许通过指定的字符设备执行命令字符串，以及命令的输入输出.它还提供了注册、注销和管理命令的 API.

通过命令管理器可以使用的几种类型的命令包括: 

1. 架构特定命令
2. 通用命令
3. 虚拟 I/O 相关命令
4. 设备驱动相关命令
5. 网络相关命令
6. 文件系统相关命令
7. ----- Many More -----

## 第12章: 存储虚拟化

Xvisor 中的存储虚拟化非常简单且轻量.它有两个关键实体: vmm_blockdev 和 vmm_vdisk.

Xvisor 的块设备框架将是 Xvisor 存储虚拟化中最关键的部分.
vmm_blockdev 是在 Xvisor 设备驱动框架的块类下注册的块设备实例的逻辑表示.
每个 vmm_blockdev 都与由 vmm_request_queue 实体表示的请求队列相关联.
所有对 vmm_blockdev 的 IO 操作都是异步的，并以 vmm_request 实体的形式提交.
vmm_blockdev 的分区将表示为具有与父 vmm_blockdev 相同的 vmm_request_queue 的子 vmm_blockdev 实例.

实现存储虚拟化并不强制需要文件系统.实际上，Xvisor 中的文件系统库（称为“VFS”）完全是可选的，仅用于加载客户镜像、脚本和日志记录.

Xvisor 中的磁盘控制器仿真器为每个客户实例的虚拟磁盘创建一个 vmm_vdisk 实例.
vmm_vdisk 是在 vmm_blockdev 实例之上的逻辑包装器，在客户创建时或使用 Xvisor 命令在运行时必须附加到 vmm_blockdev 实例.
如果附加到 vmm_vdisk 的 vmm_blockdev 在运行时被注销，则它将自动与 vmm_vdisk 分离.
如果 vmm_vdisk 没有附加到任何 vmm_blockdev，则所有对 vmm_vdisk 的 IO 请求都将失败.
此外，vmm_vdisk 的块大小必须是 vmm_blockdev 块大小的倍数.

第13章: 网络虚拟化

Xvisor 中的网络虚拟化以轻量级数据包交换框架的形式提供.
它主要提供了使用某些数据包交换策略共享宿主网络接口的抽象.
因为我们不需要完整的网络堆栈就可以提供网络虚拟化，所以 Xvisor 的网络堆栈（或 netstack 或网络套接字库）完全是可选的.
只有基于网络的管理守护程序需要网络套接字 API，并且在大多数用例中，我们可以禁用这些守护程序.

总的来说，Xvisor 的网络支持包括四个关键组件: 

1. 网络切换框架
   (Located under: <xvisor_source>/core/net)

   Xvisor 网络的主要思想是拥有一个快速的数据包交换框架.网络核心实现了 vmm_mbuf、vmm_netswitch 和 vmm_netport.

   vmm_mbuf 是数据包的 BSD 式表示.它是非常通用的数据包表示，实际上我们可以用 Xvisor vmm_mbuf 来表示 Linux sk_buff.

   vmm_netswitch 是一个仿真的网络交换机，可以连接多个 vmm_netport.
   vmm_netswitch 可以有不同的策略，如: 集线器、桥接器、路由器、VLAN 交换机等.
   目前，我们有 MAC 级桥接策略和集线器/中继策略可用.
   可以在 Xvisor 启动时或使用管理终端的命令创建具有给定策略的 vmm_netswitch.

   vmm_netport 是 vmm_netswitch 和驱动器或仿真器或 netstack 之间的逻辑连接.
   未连接到任何 vmm_netswitch 的 vmm_netport 将丢弃数据包.

2. 网络设备驱动程序
   (Located under: <xvisor_source>/drivers/net)

   宿主网络设备驱动程序将创建 vmm_netport 并将其连接到 vmm_netswitch.在这种情况下，
   vmm_netport 的 MAC 地址将与宿主网络设备的实际 MAC 地址相同.
   为网络设备驱动程序移植提供的 Linux 兼容 API 将提供: 使用 vmm_netport 的 “struct net_device” 和使用 vmm_mbuf 的 “struct sk_buff”.

   我们从不为宿主网络设备的 vmm_netports 分配任何 IP 地址.
   这些 vmm_netports 以混杂模式运行，接受任何目的 IP 地址的数据包.
   vmm_netswitch 将根据其交换策略决定转发数据包到哪些 vmm_netports.

3. 网络设备仿真器
   (Located under: <xvisor_source>/emulators/net)

   网络设备仿真器也会创建 vmm_netport 并将其连接到 vmm_netswitch.
   所有在此 vmm_netport 上接收的数据包都将被客户操作系统接收，所有由客户操作系统传输的数据包都将通过此 vmm_netport 传输.
   vmm_netport 的 MAC 地址在客户创建时指定或使用随机数生成.vmm_netport 的 IP 地址在客户操作系统中分配，Xvisor 对此不了解.

4. 可选的网络堆栈（或 netstack 或网络套接字库）
   (Located under: <xvisor_source>/libs/netstack)

   如上所述，网络堆栈（或 netstack 或网络套接字库）完全是可选的，
   我们可以将任何与 GPLv2 兼容的网络堆栈集成到 Xvisor 中，
   前提是它实现了在 libs/include/libs/netstack.h 中定义的 API.

   通常，网络堆栈将使用一个或多个伪 vmm_netports 实现，这些伪 vmm_netports 连接到一个 vmm_netswitch.
   伪 vmm_netports 的 MAC 地址可以在 Xvisor 启动时指定或使用随机数生成.
   目前，我们选择 lwIP 作为我们的可选网络堆栈，将来我们可能会采取其他措施，但 libs/netstack.h 的 API 将保持不变.

   当在 Xvisor 中启用网络堆栈时，它还将需要为其每个 vmm_netport 分配 IP 地址和其他网络设置，
   因此我们有 “net” 命令.我们也可以从外部或客户操作系统 “ping” 分配给网络堆栈的 vmm_netports 的 IP 地址.

## 第14章: 虚拟化 I/O 框架

除了存储和网络虚拟化外，还有许多类型的 I/O 需要进行虚拟化，以更好地共享主机硬件，例如: 

1. 串口
2. 输入设备（例如键盘、鼠标、多点触控、操纵杆等）
3. 显示设备（例如帧缓冲、LED 等）
4. USB 设备
5. CAN 总线
6. 传感器设备（例如 GPS、温度、陀螺仪等）
7. ----- Many More -----

我们无法针对上述所有设备都使用通用的虚拟化框架，因此在 Xvisor 中，我们有基于 I/O 设备类型的专门虚拟化框架.
这些虚拟化 I/O 框架充当了客户端仿真设备和主机设备之间的桥梁，从而实现了在多个客户实例之间共享主机设备.

目前，我们有以下虚拟化 I/O 框架: 

vmm_vserial

虚拟串口子系统由两个实体组成，即 vmm_vserial 和 vmm_vserial_receiver.
vmm_vserial 是虚拟串口的逻辑表示.客户串口仿真器为每个客户串口创建一个 vmm_vserial 实例.
发送到 vmm_vserial 实例的所有字符都由相应的客户接收，从客户接收到的所有字符都由 vmm_vserial 实例接收.
串口捕获守护程序将 vmm_vserial_receiver 实例注册到 vmm_vserial 实例以从 vmm_vserial 实例接收字符.
这些守护程序可以使用 send API 将字符发送到 vmm_serial 实例.

vmm_vinput

虚拟输入子系统由两个实体组成，即 vmm_vkeyboard 和 vmm_vmouse.
客户键盘仿真器将为每个客户键盘创建一个 vmm_vkeyboard 实例，而客户鼠标仿真器将为每个客户鼠标创建一个 vmm_vmouse 实例.
显示守护程序可以将键盘按键事件和鼠标移动事件注入到 vmm_vkeyboard 实例和 vmm_vmouse 实例中.
vmm_vkeyboard 实例和 vmm_vmouse 实例接收到的所有事件都被注入到客户作为虚拟按键事件和虚拟鼠标移动事件.

vmm_vdisplay

虚拟显示子系统有两个重要实体，即 vmm_vdisplay 和 vmm_surface.
GUI 渲染守护程序（例如 VNC 守护程序或 VScreen 守护程序等）创建 vmm_surface 实例并将其添加/绑定到 vmm_vdisplay 实例.
一个 vmm_vdisplay 实例可以由多个 GUI 渲染守护程序添加其 vmm_surface 实例.
GUI 渲染守护程序还将定期更新/同步 vmm_surface 实例与 vmm_vdisplay 实例的相关 API.
显示仿真器创建 vmm_vdisplay 实例以模拟虚拟显示.
这些显示仿真器还将使用 vmm_vdisplay 的与 surface 相关的 API 向 vmm_surface 实例提供关于虚拟显示变化的提示.
